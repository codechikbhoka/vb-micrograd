# Neural Network Implementation

A Python implementation of a neural network from scratch, demonstrating the fundamentals of deep learning with a focus on backpropagation and gradient descent.

## Overview

This project implements a neural network from scratch using Python and NumPy. It includes implementations of forward propagation, backward propagation, and various activation functions. The network is designed to be flexible and can be used for both classification and regression tasks.

## Installation

1. Clone the repository:
```bash
git clone git@github.com:codechikbhoka/vb-micrograd.git
cd neural-network
```

2. Install required packages:
```bash
pip install -r requirements.txt
```

## Project Structure

```
neural-network/
|── .vscode             # VSCode settings
|── assets              # Assets for the project
|── .cursorrules        # Cursor rules
|── .gitignore          # Git ignore file
├── README.md           # This file
├── requirements.txt    # Project dependencies
├── script.ipynb        # Main jupyter notebook which contains the code for the neural network
```

## Key Components

### Neural Network Class

The main neural network implementation includes:
- 0: Installations and Imports
- 1: Defining a function
- 2: Derivative of a function
- 3: Value class
- 4: Visualizing the lineage of the Value class's object
- 5: Understanding partial derivatives i.e. grad
- 6: Visualizing the tanh function
- 7: Building a neuron
- 8: Manual Back-Propagation
- 9: Automated Back-Propagation
- 10: PyTorch
- 11: Building a neural network


## Usage

Here's a basic example of how to use the neural network:


## Required Packages
- numpy
- matplotlib
- graphviz
- torch

## Notes

- The implementation focuses on educational purposes and may not be optimized for production use
- For large-scale applications, consider using established frameworks like TensorFlow or PyTorch
- The code includes detailed comments explaining the mathematics behind neural networks

## Acknowledgments

- Special thanks to me for completing this exercise :)
- I followed this tutorial by Andrej Karpathy: [micrograd](https://youtu.be/VMj-3S1tku0?si=igwX_ftoAF2DONBd)

## Contact

For questions and feedback, please open an issue in the GitHub repository.