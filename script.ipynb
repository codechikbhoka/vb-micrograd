{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - Installations and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !port install graphviz\n",
    "# # port is brother of brew. \n",
    "# # Somehow 'brew install graphviz' is not working seamlessly.\n",
    "# # So we are using port to install graphviz.\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from graphviz import Digraph\n",
    "import torch\n",
    "import random\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Defining a function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function f(x) = 3x^2 - 4x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 3*x**2 -4*x + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(3.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create an array of x values from -5 to 5 with a step of 0.25. Let's call this array xs.\n",
    "\n",
    "Also, let's create an array of y values by applying the function f to each x value in xs. Let's call this array ys.\n",
    "\n",
    "Finally, let's plot the values in xs and ys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "gt_ys = f(xs)\n",
    "plt.plot(xs, gt_ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Derivative of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Definition of Derivative](assets/definition-of-derivative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets find the derivative of f(x) with respect to x at x = 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.001\n",
    "x = 3.0\n",
    "derivative = (f(x + h) - f(x)) / h\n",
    "print(derivative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating it with our knowledge of calculus\n",
    "def derivative_of_f(x):\n",
    "    return 6*x - 4\n",
    "derivative_of_f(3.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it matches. Also what this 14 means is that if we start with x = 3.0 and increase x by small unit, the value of f(x) will increase by 14 units.\n",
    "\n",
    "It tells us the impact of x on f(x) at x = 3.0. The derivative is that multiplication factor which tells us the impact of \"nudging of x\" on f(x) at some x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets get more complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d = A*b + c\n",
    "\n",
    "print(d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined d as a function of multiple variables which are a, b and c.\n",
    "\n",
    "Now we will measure \n",
    "- what is the impact of a small change in a on d.\n",
    "- what is the impact of a small change in b on d.   \n",
    "- what is the impact of a small change in c on d.\n",
    "\n",
    "For sake of brevity, lets do it only for b and c.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.0001\n",
    "\n",
    "A = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d1 = A*b + c\n",
    "\n",
    "b += h\n",
    "\n",
    "d2 = A*b + c\n",
    "\n",
    "print((d2 - d1)/h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that derivative of d with respect to b at (a = 2.0, b = -3.0, c = 10.0) is 2.0.\n",
    "\n",
    "This means that if we start with b = -3.0 and increase b by small unit (0.0001), the value of d will increase by 2.0 units (i.e 2 * 0.0001).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarly, we can calculate the derivative of d with respect to c at c = 10.0.\n",
    "\n",
    "h = 0.0001\n",
    "\n",
    "A = 2.0\n",
    "b = -3.0\n",
    "c = 10.0\n",
    "\n",
    "d1 = A*b + c\n",
    "\n",
    "c += h\n",
    "\n",
    "d2 = A*b + c\n",
    "\n",
    "print((d2 - d1)/h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that derivative of d with respect to c at (a = 2.0, b = -3.0, c = 10.0) is 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #D67229;\">We performed above activity to understand the essence of derivative of a function.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Value class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define the Value class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _children=(), op='', label=''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0 # the derivative of the output with respect to this value. Notice that grad of 0 means that the output is not impacted by change in this value.\n",
    "        self._prev = set(_children)\n",
    "        self._op = op\n",
    "        self.label = label.upper()\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def print_children(self):\n",
    "        return ' & '.join(str(item.data) for item in self._prev)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        output_node = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        # backward function of output_node when output_node is the result of 'addition' of child1(i.e. self) and child2(i.e. other)\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * output_node.grad  # why += and not =, explanation is in the next cell\n",
    "            other.grad += 1.0 * output_node.grad\n",
    "        output_node._backward = _backward\n",
    "\n",
    "        return output_node\n",
    "\n",
    "    def __radd__(self, other): # other + self\n",
    "        return self + other\n",
    "\n",
    "    def __neg__(self):  # -self\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):  # self - other\n",
    "        return self + (-other)\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        output_node = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * output_node.grad\n",
    "            other.grad += self.data * output_node.grad\n",
    "        output_node._backward = _backward\n",
    "\n",
    "        return output_node\n",
    "    \n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "        output_node = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other * (self.data**(other - 1)) * output_node.grad\n",
    "\n",
    "        output_node._backward = _backward\n",
    "\n",
    "        return output_node\n",
    "\n",
    "    def __rmul__(self, other):  # other * self\n",
    "        return self * other\n",
    "\n",
    "    def __truediv__(self, other):  # self / other\n",
    "        return self * other**-1\n",
    "\n",
    "\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)\n",
    "        output_node = Value(t, (self, ), 'tanh')\n",
    "\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t**2) * output_node.grad\n",
    "        output_node._backward = _backward\n",
    "\n",
    "        return output_node\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        output_node = Value(math.exp(x), (self, ), 'exp')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad = output_node.data * output_node.grad\n",
    "\n",
    "        output_node._backward = _backward\n",
    "\n",
    "        return output_node\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1.0\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "A = Value(2.0, label='A')\n",
    "B = Value(-3.0, label='B')\n",
    "C = Value(10.0, label='C')\n",
    "E = A*B; E.label = 'E'\n",
    "D = E + C; D.label = 'D'\n",
    "F = Value(-2.0, label='F')\n",
    "L = D * F; L.label = 'L'\n",
    "\n",
    "# f is a python construct which is used to produce a formatted string. don't confuse it with function f()\n",
    "print(f'L         : {L}')\n",
    "print(f'children  : {L.print_children()}')\n",
    "print(f'operation : {L._op}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have created this Value class\n",
    "# - to act as a container for simple numbers/floats.\n",
    "# - whenever two values are added/multiplied, a new value object is generated. We want to keep track of the operation that \n",
    "#   generated this new value and the children that were used to generate this new value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Visualizing the lineage of the Value class's object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need a way to visualize the lineage of the object of this Value class.\n",
    "# - We will create a function trace() which will take a Value object and return a set of nodes and edges.\n",
    "# - Nodes are the Value objects and edges are the operations that generated the Value object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trace(root):\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    format: png | svg | ...\n",
    "    rankdir: TB (top to bottom graph) | LR (left to right)\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = Digraph(format=format, graph_attr={'rankdir': rankdir, 'pad': '0.5',  'bgcolor': '#DEE5D4'}, node_attr={'shape': 'Mrecord', 'style': 'filled', 'fillcolor': '#D2E0FB', 'fontcolor': '#003161', 'fontname': 'Roboto'})\n",
    "    \n",
    "    for n in nodes:\n",
    "        dot.node(name=str(id(n)), label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "    \n",
    "    for n1, n2 in edges:\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "    \n",
    "    return dot\n",
    "\n",
    "\n",
    "draw_dot(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Understanding partial derivatives i.e. grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.grad = 1.0 \n",
    "# It's kinda obvious and dumb that the ratio of the change in L with respect to change in itself will always be 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lol():\n",
    "    h = 0.0001\n",
    "\n",
    "    A = Value(2.0, label='A')\n",
    "    B = Value(-3.0, label='B')\n",
    "    C = Value(10.0, label='C')\n",
    "    E = A*B; E.label = 'E'\n",
    "    D = E + C; D.label = 'D'\n",
    "    F = Value(-2.0, label='F')\n",
    "    L1 = D * F; L1.label = 'L1'\n",
    "\n",
    "    A = Value(2.0 + h, label='A')   # we are nudging a by h\n",
    "    B = Value(-3.0, label='B')\n",
    "    C = Value(10.0, label='C')\n",
    "    E = A*B; E.label = 'E'\n",
    "    D = E + C; D.label = 'D'\n",
    "    F = Value(-2.0, label='F')\n",
    "    L2 = D * F; L2.label = 'L2'\n",
    "\n",
    "    print('derivative of l with respect to a: ', (L2.data - L1.data)/h)\n",
    "\n",
    "\n",
    "lol()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This 6 tells us that change in 'l' will be 6 times the change in 'a' when we are making that small change at (a = 2.0, b = -3.0, c = 10.0).\n",
    "\n",
    "\n",
    "# Now we are going to find derivative of L with respect to all the variables.\n",
    "\n",
    "# Since l = d * f\n",
    "# => dl/dd = f and \n",
    "# => dl/df = d\n",
    "\n",
    "# So D.grad will be equal to F.data and F.grad will be equal to D.data.\n",
    "\n",
    "# Remember, D.grad tells us the impact of change in d on l. Change in l will be that much times the change in d.\n",
    "\n",
    "# Similarly, F.grad tells us the impact of change in f on l. Change in l will be that much times the change in f.\n",
    "\n",
    "\n",
    "# Now to find dl/dc, we need to use chain rule. Impact of change in c on l will be the (impact of change in c on d) times the (impact of change in d on l).\n",
    "\n",
    "# dl/dc = dd/dc * dl/dd\n",
    "\n",
    "# We already know dl/dd, we just need to find dd/dc.\n",
    "\n",
    "# Since d = e + c\n",
    "# => dd/dc = 1.0\n",
    "\n",
    "# So dl/dc = 1.0 * dl/dd = dl/dd\n",
    "\n",
    "# Similarly, dl/de = dl/dd\n",
    "\n",
    "\n",
    "# Now this statement is the crux of back-propagation. \n",
    "\n",
    "# When there is a parent node p and it has two child nodes c1 and c2,\n",
    "# and we already know p.data, c1.data and c2.data,\n",
    "# and also p.grad,\n",
    "# then we can find c1.grad and c2.grad using the following formula:\n",
    "\n",
    "# if(c1.data and c2.data were multiplied to derive p.data)\n",
    "#     c1.grad = p.grad * c2.data\n",
    "#     c2.grad = p.grad * c1.data\n",
    "\n",
    "# else if(c1.data and c2.data were added to derive p.data)\n",
    "#     c1.grad = p.grad\n",
    "#     c2.grad = p.grad\n",
    "\n",
    "# This is the essence of back-propagation. We are able to the calculate grads in previous layer using the grads in the current layer and data in the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Visualizing the tanh function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot the tanh function to visualize its behavior.\n",
    "xs = np.arange(-5, 5, 0.2)\n",
    "gt_ys = np.tanh(xs)\n",
    "plt.plot(xs, gt_ys); \n",
    "plt.grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Derivative of tanh\n",
    "![Derivative of tanh](assets/derivative-of-tanh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Building a neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Till now, we have been building our understanding about a function and essence of its derivative. We created a nice Value class to ease our handling of nodes which contains data, grads and children. We went through the process of forward propagation where we calculated the final L value using the input variables a,b and c. Also we went through the process of back-propagation where we calculated the grads of L with respect to all the variables.\n",
    "\n",
    "# But we have been using a random function to calculate L using a, b and c. Now we are going to create a neural network. For that we need to use a function which is not random but a function which is used in practice.\n",
    "\n",
    "# We will use a function which is used in practice.\n",
    "\n",
    "# We will first create a simple neuron whose data is going to be calculated using \n",
    "# - data in neurons in its previous layer (x1, x2),\n",
    "# - weights of the edges which connects those two neurons to this neuron (w1, w2),\n",
    "# - bias attribute of this neuron (b).\n",
    "\n",
    "# We will then apply the tanh function on the data of this neuron.\n",
    "\n",
    "# We will then visualize the lineage of this neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "X1 = Value(2.0, label='X1')\n",
    "X2 = Value(0.0, label='X2')\n",
    "\n",
    "# weights w1,w2\n",
    "W1 = Value(-3.0, label='W1')\n",
    "W2 = Value(1.0, label='W2')\n",
    "\n",
    "# bias of the neuron\n",
    "B = Value(6.8813735870195432, label='B') # bias could have been any other value like 1, 4, 10, 50 etc. We are using this value so that overall number which are going to be output for this bias are sane and are not crazy numbers. I think Andrej has found this number using hit and trial, idk\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "X1W1 = X1*W1; X1W1.label = 'X1*W1'\n",
    "X2W2 = X2*W2; X2W2.label = 'X2*W2'\n",
    "X1W1X2W2 = X1W1 + X2W2; X1W1X2W2.label = 'X1*W1 + X2*W2'\n",
    "\n",
    "N = X1W1X2W2 + B; N.label = 'N'\n",
    "O = N.tanh(); O.label = 'O'\n",
    "\n",
    "draw_dot(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8 - Manual Back-Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be doing back-propagation manually now. We will calculate the grads of all the neurons in the network, going backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O.grad = 1.0\n",
    "\n",
    "draw_dot(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since o = tanh(n)\n",
    "# => do/dn = tanh'(n)\n",
    "# => do/dn = 1 - tanh(n)^2\n",
    "# => do/dn = 1 - o^2\n",
    "# => N.grad = 1 - o^2\n",
    "\n",
    "N.grad = 1 - O.data**2 # this comes out to be 0.5\n",
    "\n",
    "draw_dot(O)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1W1X2W2.grad = N.grad\n",
    "B.grad = N.grad\n",
    "\n",
    "draw_dot(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since + operator was used to derive X1W1X2W2,  the grad of X1W1 and X2W2 will be the same as the grad of X1W1X2W2.\n",
    "X1W1.grad = X1W1X2W2.grad\n",
    "X2W2.grad = X1W1X2W2.grad\n",
    "\n",
    "draw_dot(O)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# since * operator was used to derive X1W1, the grad of W1 will be the product of the data of X1 and the grad of X1W1.\n",
    "W1.grad = X1.data * X1W1.grad\n",
    "# since * operator was used to derive X1, the grad of X1 will be the product of the data of W1 and the grad of X1W1.\n",
    "X1.grad = W1.data * X1W1.grad\n",
    "\n",
    "\n",
    "# since * operator was used to derive X2W2, the grad of W2 will be the product of the data of X2 and the grad of X2W2.\n",
    "W2.grad = X2.data * X2W2.grad\n",
    "# since * operator was used to derive X2, the grad of X2 will be the product of the data of W2 and the grad of X2W2.\n",
    "X2.grad = W2.data * X2W2.grad\n",
    "\n",
    "draw_dot(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 - Automated Back-Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets build the neuron again and do the forward propagation.\n",
    "\n",
    "# inputs x1,x2\n",
    "X1 = Value(2.0, label='X1')\n",
    "X2 = Value(0.0, label='X2')\n",
    "\n",
    "# weights w1,w2\n",
    "W1 = Value(-3.0, label='W1')\n",
    "W2 = Value(1.0, label='W2')\n",
    "\n",
    "# bias of the neuron\n",
    "B = Value(6.8813735870195432, label='B') # bias could have been any other value like 1, 4, 10, 50 etc. We are using this value so that overall number which are going to be output for this bias are sane and are not crazy numbers. I think Andrej has found this number using hit and trial, idk\n",
    "\n",
    "# x1*w1 + x2*w2 + b\n",
    "X1W1 = X1*W1; X1W1.label = 'X1*W1'\n",
    "X2W2 = X2*W2; X2W2.label = 'X2*W2'\n",
    "X1W1X2W2 = X1W1 + X2W2; X1W1X2W2.label = 'X1*W1 + X2*W2'\n",
    "\n",
    "N = X1W1X2W2 + B; N.label = 'N'\n",
    "O = N.tanh(); O.label = 'O'\n",
    "\n",
    "draw_dot(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will do the automated back-propagation.\n",
    "O.backward()\n",
    "draw_dot(O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole of these calculations were for 1 neuron. This neuron in layer l had two neurons in layer l-1 as its children. Those neurons were X1 and X2.\n",
    "\n",
    "# In above process, we calculated gradients of the final output O with respect to W1, W2 and B.\n",
    "\n",
    "# We also calculated gradients of the final output O with respect to X1 and X2. These gradients are of no use if X1 and X2 are in the very first layer. But if they lie in some intermediate layer, then we can use these gradients to update the weights of the edges which connect the neurons in the further previous layer (layer l-2) to the neurons in the current layer (layer l-1).\n",
    "\n",
    "# This is the essence of back-propagation. We are able to calculate the gradients of the neurons in the previous layer using the gradients in the current layer and the data in the previous layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note 1:\n",
    "# Why we should do self.grad += ... and not simply self.grad = ...\n",
    "# i.e. why use += and not just =\n",
    "M = Value(5.0, label='M')\n",
    "N = M + M;     N.label = 'N'\n",
    "\n",
    "N.backward()\n",
    "draw_dot(N)\n",
    "\n",
    "# Ideally M.grad should be 2.0. But if we use only = instead of +=, then M.grad will come out to be only 1.0.\n",
    "# This is because when we use =, we are overwriting the value of M.grad with 1.0.\n",
    "# But when we use +=, we are adding 1.0 to the existing value of M.grad.\n",
    "# So M.grad becomes 1.0 + 1.0 = 2.0.\n",
    "# This is why we should use += instead of =\n",
    "\n",
    "# This fact is explained well at this timestamp: https://youtu.be/VMj-3S1tku0?si=XP4SbULPCYFR0n87&t=4948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 - PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch is a modern deep learning library. It is built on top of Torch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scalars**\n",
    "\n",
    "* A scalar is a single number.\n",
    "\n",
    "\n",
    "**Vectors**\n",
    "\n",
    "* **One-dimensional arrays:** Vectors are essentially lists or arrays of numbers arranged in a specific order. \n",
    "\n",
    "**Tensors**\n",
    "\n",
    "* **Generalized arrays:** Tensors are a more general concept that encompass vectors as a special case. \n",
    "* **Multidimensional arrays:** They can be thought of as multidimensional arrays of numbers. \n",
    "* **Rank:** The \"order\" or \"rank\" of a tensor refers to the number of dimensions it has.\n",
    "    * **Rank-0 tensor:** A scalar (single number)\n",
    "    * **Rank-1 tensor:** A vector\n",
    "    * **Rank-2 tensor:** A matrix\n",
    "    * **Higher-rank tensors:** Arrays with more than two dimensions\n",
    "\n",
    "**Key Differences**\n",
    "\n",
    "* **Dimensionality:** Vectors are always one-dimensional, while tensors can have any number of dimensions.\n",
    "* **Generality:** Tensors are a more general mathematical concept that encompass vectors as a specific type.\n",
    "\n",
    "**In simpler terms:**\n",
    "\n",
    "Imagine a vector as a single row or column of numbers. A tensor is like an extension of this, where you can have multiple rows and columns, or even more complex arrangements depending on its rank.\n",
    "\n",
    "**Applications**\n",
    "\n",
    "* **Physics:** Tensors are fundamental in physics, describing quantities like stress, strain, and electromagnetic fields.\n",
    "* **Machine learning:** Tensors are widely used in deep learning for representing data like images, videos, and text.\n",
    "\n",
    "**In Summary**\n",
    "\n",
    "Vectors are a specific type of tensor, specifically a rank-1 tensor. Tensors provide a more general framework for representing and manipulating multidimensional data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.Tensor([2.0]).double()\n",
    "x2 = torch.Tensor([0.0]).double()\n",
    "w1 = torch.Tensor([-3.0]).double()\n",
    "w2 = torch.Tensor([1.0]).double()\n",
    "b = torch.Tensor([6.8813735870195432]).double()\n",
    "\n",
    "x1.requires_grad = True\n",
    "x2.requires_grad = True\n",
    "w1.requires_grad = True\n",
    "w2.requires_grad = True\n",
    "b.requires_grad = True\n",
    "\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "\n",
    "o.backward()\n",
    "\n",
    "print('-')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we see that PyTorch has same datastructures as we have defined in our Value class with same functionality. Pytorch is more advanced and has more features in addition to the functionality we have defined in our Value class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 - Building a neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Lets build a Neuron with random weights and bias configured for it.\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, num_inputs):\n",
    "        self.w = [Value(random.uniform(-1,1)) for _ in range(num_inputs)]\n",
    "        self.b = Value(random.uniform(-1,1))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # w * x + b\n",
    "        act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        out = act.tanh()\n",
    "        return out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "# x stores the data of neurons in the previous layer.\n",
    "x = [2.0, 3.0]\n",
    "\n",
    "# we create a neuron which has 2 input edges, and hence 2 weights configured for it.\n",
    "n = Neuron(2)\n",
    "\n",
    "# we pass the data of neurons in the previous layer to this neuron.\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define a layer of neurons.\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        # we create num_outputs neurons, each with num_inputs input edges.\n",
    "        self.neurons = [Neuron(num_inputs) for _ in range(num_outputs)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "x = [2.0, 3.0] # previous layer has 2 neurons, hence 2 elements in the list\n",
    "l = Layer(2, 3) # each neuron in this layer has 2 input edges because previous layer has 2 neurons. This layer has 3 neurons.\n",
    "l(x)\n",
    "\n",
    "# Now we have a layer of neurons. We can pass the data of neurons in the previous layer to this layer and get the data of neurons in the current layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets create a multi-layer perceptron.\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, num_inputs, num_outputs_in_each_layer):\n",
    "        sz = [num_inputs] + num_outputs_in_each_layer\n",
    "        self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(num_outputs_in_each_layer))]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for neuron in layer.neurons for p in neuron.parameters()]\n",
    "\n",
    "x = [2.0, 3.0, -1.0]\n",
    "m = MLP(3, [4, 4, 1])\n",
    "\n",
    "m(x) # output of the multi-layer perceptron when input is x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram of a multi-layer perceptron](assets/neural-network-diagram.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "    [2.0, 3.0, -1.0],\n",
    "    [3.0, -1.0, 0.5],\n",
    "    [0.5, 1.0, 1.0],\n",
    "    [1.0, 1.0, -1.0]\n",
    "]\n",
    "\n",
    "gt_ys = [1.0, -1.0, 1.0, -1.0]  # ground truth ys\n",
    "\n",
    "pred_ys = [m(x) for x in xs] # predicted ys\n",
    "\n",
    "pred_ys\n",
    "\n",
    "loss = sum((pred_y - gt_y)**2 for pred_y, gt_y in zip(pred_ys, gt_ys))\n",
    "\n",
    "loss\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "m.layers[0].neurons[0].w[0].grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just ran the forward propagation on 4 data inputs (i.e. xs) and got 4 predicted outputs (i.e. pred_ys).\n",
    "\n",
    "# We then calculated the loss between the predicted outputs and the ground truth outputs (i.e. gt_ys).\n",
    "\n",
    "# We then ran the back-propagation on the loss to calculate the gradients of the parameters of the neural network with respect to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will update the parameters of the neural network using the gradients and a learning rate.\n",
    "\n",
    "# We will repeat the process for multiple iterations to reach to a good enough configuration of the parameters, so that the loss is minimized.\n",
    "\n",
    "# This is what we call training a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will notice that we will decrease every parameter by a small amount and that amount will be proportional to the gradient of the parameter with respect to the loss.\n",
    "\n",
    "# What's the logic behind decreasing the parameter by that much proportion only ?\n",
    "\n",
    "# This is where gradient descent comes into play.\n",
    "\n",
    "# let X = (x1, x2, x3, ... xn)\n",
    "# if y = f(X)\n",
    "# then we first calculate the gradient of y with respect to each of the xi.\n",
    "# i.e. dy/dx1, dy/dx2, dy/dx3, ... dy/dxn\n",
    "# this vector of gradients is called the gradient of y with respect to the xi's. lets call this X'\n",
    "\n",
    "# if we increase x1 by g1, x2 by g2, x3 by g3, ... xn by gn, then there will be change in y.\n",
    "# There are so many possible ways to increase each of the xi.\n",
    "# But the claim is that the best way (the one which results maximum increment in y) to increase each of the xi is to increase each of the xi by the gradient of y with respect to the xi.\n",
    "\n",
    "# To understand its proof, watch these videos:\n",
    "# - https://youtu.be/tIpKfDc295M?si=SnzgxoPD9jzx_ydV\n",
    "# - https://youtu.be/_-02ze7tf08?si=4IuFEN3TpEYbgZfM\n",
    "# - https://youtu.be/TEB2z7ZlRAw?si=uKBCsZ9fHFuJcqHI\n",
    "# - https://youtu.be/ZTbTYEMvo10?si=EzLFgw_33nopNLXr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will iterate the process of forward propagation, back-propagation and parameter update for 2000 iterations.\n",
    "\n",
    "loss_tracker = []\n",
    "\n",
    "for k in range(2000):\n",
    "  # forward pass\n",
    "  # it updates the data in all the neurons in the neural network.\n",
    "  pred_ys = [m(x) for x in xs]\n",
    "  loss = sum((pred_y - gt_y)**2 for pred_y, gt_y in zip(pred_ys, gt_ys))\n",
    "  \n",
    "  # backward pass\n",
    "  # it updates the gradients of all the parameters in the neural network.\n",
    "  for p in m.parameters():\n",
    "    p.grad = 0.0\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  # it updates the data in all the parameters in the neural network.\n",
    "  for p in m.parameters():\n",
    "    p.data += -0.001 * p.grad\n",
    "  \n",
    "  print(k, loss.data)\n",
    "\n",
    "  loss_tracker.append(loss.data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_tracker)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gt_ys)\n",
    "print([pred_y.data for pred_y in pred_ys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that the predicted outputs are very close to the ground truth outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final setting of the parameters of the neural network.\n",
    "print('number of parameters: ', len(m.parameters()))\n",
    "print('parameters: ', [p.data for p in m.parameters()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
